{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-primary alert-info\">\n",
    "    \n",
    "# Reinforcement Learning - Mountain Car\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- ### SARSA ($\\lambda = 0$)\n",
    "\n",
    "- ### Q-Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/MountainCar_T0.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Episode Terminated when:\n",
    "- Car position $\\gt$ 0.5\n",
    "- Episode length $\\gt$ 200\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA_Agent:\n",
    "\n",
    "    def __init__(self, env, num_bins, max_episodes):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.num_bins = num_bins\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_episode_length = 200\n",
    "\n",
    "        self.position_array = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=self.num_bins, endpoint=True)\n",
    "        self.velocity_array = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=self.num_bins, endpoint=True)\n",
    "\n",
    "        self.action_state_values = np.zeros((self.num_bins + 1, self.num_bins + 1, env.action_space.n))\n",
    "        self.policy = np.random.randint(low=0, high=env.action_space.n, size=(self.num_bins + 1, self.num_bins + 1))\n",
    "\n",
    "        self.alpha = 0.05\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = 500 * self.epsilon_min / (self.max_episodes * self.max_episode_length)\n",
    "\n",
    "    def digitize(self, obs):\n",
    "        return np.digitize(obs[0], self.position_array), np.digitize(obs[1], self.velocity_array)\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        discrete_obs = np.digitize(obs[0], self.position_array), np.digitize(obs[1], self.velocity_array)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return self.policy[discrete_obs]\n",
    "        else:\n",
    "            return np.random.choice(env.action_space.n)\n",
    "\n",
    "    def learn(self, obs, action, reward, next_obs):\n",
    "        discrete_obs = np.digitize(obs[0], self.position_array), np.digitize(obs[1], self.velocity_array)\n",
    "        discrete_next_obs = np.digitize(next_obs[0], self.position_array), np.digitize(next_obs[1], self.velocity_array)\n",
    "        next_action = self.policy[discrete_next_obs]\n",
    "        action_values = reward + \\\n",
    "                        self.gamma * self.action_state_values[discrete_next_obs][next_action] - \\\n",
    "                        self.action_state_values[discrete_obs][action]\n",
    "        self.action_state_values[discrete_obs][action] += self.alpha * action_values\n",
    "        self.policy[discrete_obs] = np.argmax(self.action_state_values[discrete_obs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent:\n",
    "\n",
    "    def __init__(self, env, num_bins, max_episodes):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.num_bins = num_bins\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_episode_length = 200\n",
    "\n",
    "        self.position_array = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=self.num_bins, endpoint=True)\n",
    "        self.velocity_array = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=self.num_bins, endpoint=True)\n",
    "\n",
    "        self.action_state_values = np.zeros((self.num_bins + 1, self.num_bins + 1, env.action_space.n))\n",
    "\n",
    "        self.alpha = 0.05\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = 500 * self.epsilon_min / (self.max_episodes * self.max_episode_length)\n",
    "\n",
    "    def digitize(self, obs):\n",
    "        return np.digitize(obs[0], self.position_array), np.digitize(obs[1], self.velocity_array)\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        discrete_obs = np.digitize(obs[0], self.position_array), np.digitize(obs[1], self.velocity_array)\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return np.argmax(self.action_state_values[discrete_obs])\n",
    "        else:\n",
    "            return np.random.choice(env.action_space.n)\n",
    "\n",
    "    def learn(self, obs, action, reward, next_obs):\n",
    "        discrete_obs = np.digitize(obs[0], self.position_array), np.digitize(obs[1], self.velocity_array)\n",
    "        discrete_next_obs = np.digitize(next_obs[0], self.position_array), np.digitize(next_obs[1], self.velocity_array)\n",
    "        action_values = reward + \\\n",
    "                        self.gamma * np.max(self.action_state_values[discrete_next_obs]) - \\\n",
    "                        self.action_state_values[discrete_obs][action]\n",
    "        self.action_state_values[discrete_obs][action] += self.alpha * action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent):\n",
    "    best_reward = np.finfo(np.float64).min\n",
    "    for episode_idx in range(agent.max_episodes):\n",
    "        done, total_reward = False, 0.0\n",
    "        obs = agent.env.reset()\n",
    "        while not done:\n",
    "            action = agent.select_action(obs)\n",
    "            next_obs, reward, done, info = agent.env.step(action)\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "        best_reward = max(best_reward, total_reward)\n",
    "        print('Episode: {}, Reward: {}, Best_reward: {}'.format(episode_idx+1, total_reward, best_reward))\n",
    "    return np.argmax(agent.action_state_values, axis=2)\n",
    "\n",
    "\n",
    "def test(agent, env, policy):\n",
    "    done, total_reward = False, 0.0\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = policy[agent.digitize(obs)]\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    env = gym.make('MountainCar-v0')\n",
    "\n",
    "    max_episodes = np.arange(50000, 100000, 10000, dtype=np.int32)\n",
    "    num_bins = np.arange(20, 40, 5, dtype=np.int32)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    SARSA Agent\n",
    "    \"\"\"\n",
    "    agent = SARSA_Agent(env, num_bins[0], max_episodes[0])\n",
    "    policy = train(agent)\n",
    "\n",
    "    output_dir = './sarsa_output'\n",
    "    env = gym.wrappers.Monitor(env, output_dir, force=True)\n",
    "    for _ in range(1000):\n",
    "        test(agent, env, policy)\n",
    "\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Q Agent\n",
    "    \"\"\"\n",
    "    agent = Q_Agent(env, num_bins[0], max_episodes[0])\n",
    "    policy = train(agent)\n",
    "\n",
    "    output_dir = './q_output'\n",
    "    env = gym.wrappers.Monitor(env, output_dir, force=True)\n",
    "    for _ in range(1000):\n",
    "        test(agent, env, policy)\n",
    "\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "### Q Agent\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### During training (On the way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/Q_Agent_Intermediate.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### After training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/Q_Agent.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "### SARSA Agent\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### During training (On the way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/SARSA_Agent_Intermediate.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### After training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='utils/SARSA_Agent.gif'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
